{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "number_plate_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "14P1rOzp2sgkdMPT19XUXFo65NNIBt9ek",
      "authorship_tag": "ABX9TyPIa45PjsrBeAoXcJe/tfKP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anuraagshankar/number-plate-recognition-system/blob/master/number_plate_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDvDZmqTyrvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyunpack import Archive\n",
        "Archive('/content/drive/My Drive/Number Plate Detection.rar').extractall('/content/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdqD8D6UzUNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.layers import Input, Lambda\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\n",
        "from yolo3.utils import get_random_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjiQxWtk0gPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = open('Number_Plate_Detection/OIDv4_ToolKit/4_CLASS_test.txt')\n",
        "lines = file.readlines()\n",
        "for i in range(len(lines)):\n",
        "    line = lines[i].split(' ')\n",
        "    line[0] = 'Number_Plate_Detection/OIDv4_ToolKit/OID/Dataset/train/Vehicle_registration_plate/' + line[2].split('\\\\')[-1]\n",
        "    del line[1:3]\n",
        "    lines[i] = ' '.join(line)\n",
        "    \n",
        "newFile = open('4_CLASS_test.txt', 'w')\n",
        "newFile.writelines(lines)\n",
        "newFile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmS7qdzr0isN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _main():\n",
        "    annotation_path = '4_CLASS_test.txt'\n",
        "    log_dir = 'logs/000/'\n",
        "    classes_path = 'Number_Plate_Detection/OIDv4_ToolKit/4_CLASS_test_classes.txt'\n",
        "    anchors_path = 'Number_Plate_Detection/OIDv4_ToolKit/model_data/yolo_anchors.txt'\n",
        "    class_names = get_classes(classes_path)\n",
        "    num_classes = len(class_names)\n",
        "    anchors = get_anchors(anchors_path)\n",
        "\n",
        "    input_shape = (416,416) # multiple of 32, hw\n",
        "\n",
        "    is_tiny_version = len(anchors)==6 # default setting\n",
        "    if is_tiny_version:\n",
        "        model = create_tiny_model(input_shape, anchors, num_classes,\n",
        "            freeze_body=2, weights_path='Number_Plate_Detection/OIDv4_ToolKit/model_data/yolo_weights.h5')\n",
        "    else:\n",
        "        model = create_model(input_shape, anchors, num_classes, freeze_body=2, weights_path='Number_Plate_Detection/OIDv4_ToolKit/model_data/yolo_weights.h5') # make sure you know what you freeze\n",
        "\n",
        "\n",
        "    logging = TensorBoard(log_dir=log_dir)\n",
        "    checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
        "        monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
        "\n",
        "    val_split = 0.1\n",
        "    with open(annotation_path) as f:\n",
        "        lines = f.readlines()\n",
        "    np.random.shuffle(lines)\n",
        "    num_val = int(len(lines)*val_split)\n",
        "    num_train = len(lines) - num_val\n",
        "\n",
        "    # Train with frozen layers first, to get a stable loss.\n",
        "    # Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n",
        "    if True:\n",
        "        model.compile(optimizer=Adam(lr=1e-3), loss={\n",
        "            # use custom yolo_loss Lambda layer.\n",
        "            'yolo_loss': lambda y_true, y_pred: y_pred})\n",
        "\n",
        "        batch_size = 32\n",
        "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
        "        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
        "                steps_per_epoch=max(1, num_train//batch_size),\n",
        "                validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
        "                validation_steps=max(1, num_val//batch_size),\n",
        "                epochs=50,\n",
        "                initial_epoch=0,\n",
        "                callbacks=[logging, checkpoint])\n",
        "        model.save_weights(log_dir + 'trained_weights_stage_1.h5')\n",
        "\n",
        "    # Unfreeze and continue training, to fine-tune.\n",
        "    # Train longer if the result is not good.\n",
        "    if True:\n",
        "        for i in range(len(model.layers)):\n",
        "            model.layers[i].trainable = True\n",
        "        model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n",
        "        print('Unfreeze all of the layers.')\n",
        "\n",
        "        batch_size = 8 # note that more GPU memory is required after unfreezing the body\n",
        "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
        "        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
        "            steps_per_epoch=max(1, num_train//batch_size),\n",
        "            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
        "            validation_steps=max(1, num_val//batch_size),\n",
        "            epochs=100,\n",
        "            initial_epoch=50,\n",
        "            callbacks=[logging, checkpoint, reduce_lr, early_stopping])\n",
        "        model.save_weights(log_dir + 'trained_weights_final.h5')\n",
        "\n",
        "    # Further training if needed.\n",
        "\n",
        "\n",
        "def get_classes(classes_path):\n",
        "    '''loads the classes'''\n",
        "    with open(classes_path) as f:\n",
        "        class_names = f.readlines()\n",
        "    class_names = [c.strip() for c in class_names]\n",
        "    return class_names\n",
        "\n",
        "def get_anchors(anchors_path):\n",
        "    '''loads the anchors from a file'''\n",
        "    with open(anchors_path) as f:\n",
        "        anchors = f.readline()\n",
        "    anchors = [float(x) for x in anchors.split(',')]\n",
        "    return np.array(anchors).reshape(-1, 2)\n",
        "\n",
        "\n",
        "def create_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
        "            weights_path='Number_Plate_Detection/OIDv4_ToolKit/model_data/yolo_weights.h5'):\n",
        "    '''create the training model'''\n",
        "    K.clear_session() # get a new session\n",
        "    image_input = Input(shape=(None, None, 3))\n",
        "    h, w = input_shape\n",
        "    num_anchors = len(anchors)\n",
        "\n",
        "    y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\\n",
        "        num_anchors//3, num_classes+5)) for l in range(3)]\n",
        "\n",
        "    model_body = yolo_body(image_input, num_anchors//3, num_classes)\n",
        "    print('Create YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
        "\n",
        "    if load_pretrained:\n",
        "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
        "        print('Load weights {}.'.format(weights_path))\n",
        "        if freeze_body in [1, 2]:\n",
        "            # Freeze darknet53 body or freeze all but 3 output layers.\n",
        "            num = (185, len(model_body.layers)-3)[freeze_body-1]\n",
        "            for i in range(num): model_body.layers[i].trainable = False\n",
        "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
        "\n",
        "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
        "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5})(\n",
        "        [*model_body.output, *y_true])\n",
        "    model = Model([model_body.input, *y_true], model_loss)\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_tiny_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
        "            weights_path='Number_Plate_Detection/OIDv4_ToolKit/model_data/tiny_yolo_weights.h5'):\n",
        "    '''create the training model, for Tiny YOLOv3'''\n",
        "    K.clear_session() # get a new session\n",
        "    image_input = Input(shape=(None, None, 3))\n",
        "    h, w = input_shape\n",
        "    num_anchors = len(anchors)\n",
        "\n",
        "    y_true = [Input(shape=(h//{0:32, 1:16}[l], w//{0:32, 1:16}[l], \\\n",
        "        num_anchors//2, num_classes+5)) for l in range(2)]\n",
        "\n",
        "    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\n",
        "    print('Create Tiny YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
        "\n",
        "    if load_pretrained:\n",
        "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
        "        print('Load weights {}.'.format(weights_path))\n",
        "        if freeze_body in [1, 2]:\n",
        "            # Freeze the darknet body or freeze all but 2 output layers.\n",
        "            num = (20, len(model_body.layers)-2)[freeze_body-1]\n",
        "            for i in range(num): model_body.layers[i].trainable = False\n",
        "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
        "\n",
        "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
        "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.7})(\n",
        "        [*model_body.output, *y_true])\n",
        "    model = Model([model_body.input, *y_true], model_loss)\n",
        "\n",
        "    return model\n",
        "\n",
        "def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
        "    '''data generator for fit_generator'''\n",
        "    n = len(annotation_lines)\n",
        "    i = 0\n",
        "    while True:\n",
        "        image_data = []\n",
        "        box_data = []\n",
        "        for b in range(batch_size):\n",
        "            if i==0:\n",
        "                np.random.shuffle(annotation_lines)\n",
        "            image, box = get_random_data(annotation_lines[i], input_shape, random=True)\n",
        "            image_data.append(image)\n",
        "            box_data.append(box)\n",
        "            i = (i+1) % n\n",
        "        image_data = np.array(image_data)\n",
        "        box_data = np.array(box_data)\n",
        "        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
        "        yield [image_data, *y_true], np.zeros(batch_size)\n",
        "\n",
        "def data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
        "    n = len(annotation_lines)\n",
        "    if n==0 or batch_size<=0: return None\n",
        "    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    _main()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}